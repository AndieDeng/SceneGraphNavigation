{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e233f3-2017-46d3-829f-e69a52a56da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   accelerator: GPU\n",
    "#   jupytext:\n",
    "#     cell_metadata_filter: -all\n",
    "#     formats: nb_python//py:percent,notebooks//ipynb\n",
    "#     notebook_metadata_filter: allba\n",
    "#       extension: .p\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.13.7\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3 (ipykernel)\n",
    "#     language: python\n",
    "#     name: python3\n",
    "#   language_info:\n",
    "#     codemirror_mode:\n",
    "#       name: ipython\n",
    "#       version: 3\n",
    "#     file_extension: .py\n",
    "#     mimetype: text/x-python\n",
    "#     name: python\n",
    "#     nbconvert_exporter: python\n",
    "#     pygments_lexer: ipython3\n",
    "#     version: 3.9.17\n",
    "# ---\n",
    "\n",
    "# %% [markdown]\n",
    "# #Habitat-sim ReplicaCAD Quickstart\n",
    "#\n",
    "# This brief tutorial demonstrates loading the ReplicaCAD dataset in Habitat-sim from a SceneDataset and rendering a short video of agent navigation with physics simulation.\n",
    "#\n",
    "\n",
    "# %%\n",
    "# @title Path Setup and Imports { display-mode: \"form\" }\n",
    "# @markdown (double click to show code).\n",
    "\n",
    "## [setup]\n",
    "import os\n",
    "\n",
    "import git\n",
    "import magnum as mn\n",
    "\n",
    "import habitat_sim\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display as ipydisplay\n",
    "\n",
    "    # For using jupyter/ipywidget IO components\n",
    "\n",
    "    HAS_WIDGETS = True\n",
    "except ImportError:\n",
    "    HAS_WIDGETS = False\n",
    "\n",
    "repo = git.Repo(\".\", search_parent_directories=True)\n",
    "dir_path = repo.working_tree_dir\n",
    "data_path = os.path.join(dir_path, \"data\")\n",
    "output_path = os.path.join(\n",
    "    dir_path, \"examples/tutorials/replica_cad_output/\"\n",
    ")  # @param {type:\"string\"}\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# define some globals the first time we run.\n",
    "if \"sim\" not in globals():\n",
    "    global sim\n",
    "    sim = None\n",
    "    global obj_attr_mgr\n",
    "    obj_attr_mgr = None\n",
    "    global stage_attr_mgr\n",
    "    stage_attr_mgr = None\n",
    "    global rigid_obj_mgr\n",
    "    rigid_obj_mgr = None\n",
    "\n",
    "\n",
    "# %%\n",
    "# @title Define Configuration Utility Functions { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "\n",
    "# @markdown This cell defines a number of utility functions used throughout the tutorial to make simulator reconstruction easy:\n",
    "# @markdown - make_cfg\n",
    "# @markdown - make_default_settings\n",
    "# @markdown - make_simulator_from_settings\n",
    "\n",
    "\n",
    "def make_cfg(settings):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.scene_dataset_config_file = settings[\"scene_dataset\"]\n",
    "    sim_cfg.scene_id = settings[\"scene\"]\n",
    "    sim_cfg.enable_physics = settings[\"enable_physics\"]\n",
    "    # Specify the location of the scene dataset\n",
    "    if \"scene_dataset_config\" in settings:\n",
    "        sim_cfg.scene_dataset_config_file = settings[\"scene_dataset_config\"]\n",
    "    if \"override_scene_light_defaults\" in settings:\n",
    "        sim_cfg.override_scene_light_defaults = settings[\n",
    "            \"override_scene_light_defaults\"\n",
    "        ]\n",
    "    if \"scene_light_setup\" in settings:\n",
    "        sim_cfg.scene_light_setup = settings[\"scene_light_setup\"]\n",
    "\n",
    "    # Note: all sensors must have the same resolution\n",
    "    sensor_specs = []\n",
    "    color_sensor_1st_person_spec = habitat_sim.CameraSensorSpec()\n",
    "    color_sensor_1st_person_spec.uuid = \"color_sensor_1st_person\"\n",
    "    color_sensor_1st_person_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    color_sensor_1st_person_spec.resolution = [\n",
    "        settings[\"height\"],\n",
    "        settings[\"width\"],\n",
    "    ]\n",
    "    color_sensor_1st_person_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    color_sensor_1st_person_spec.orientation = [\n",
    "        settings[\"sensor_pitch\"],\n",
    "        0.0,\n",
    "        0.0,\n",
    "    ]\n",
    "    color_sensor_1st_person_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    sensor_specs.append(color_sensor_1st_person_spec)\n",
    "\n",
    "    depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    depth_sensor_spec.uuid = \"depth_sensor\"\n",
    "    depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "    depth_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    depth_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    sensor_specs.append(depth_sensor_spec)\n",
    "\n",
    "    semantic_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    semantic_sensor_spec.uuid = \"semantic_sensor\"\n",
    "    semantic_sensor_spec.sensor_type = habitat_sim.SensorType.SEMANTIC\n",
    "    semantic_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    semantic_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    semantic_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "    sensor_specs.append(semantic_sensor_spec)\n",
    "\n",
    "    # Here you can specify the amount of displacement in a forward action and the turn angle\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])\n",
    "\n",
    "\n",
    "def make_default_settings():\n",
    "    rgb_sensor = True  # @param {type:\"boolean\"}\n",
    "    depth_sensor = True  # @param {type:\"boolean\"}\n",
    "    semantic_sensor = True  # @param {type:\"boolean\"}\n",
    "    settings = {\n",
    "        \"width\": 1280,  # Spatial resolution of the observations\n",
    "        \"height\": 720,\n",
    "        \"scene_dataset\": os.path.join(\n",
    "            data_path, \"replica_cad/replicaCAD.scene_dataset_config.json\"\n",
    "        ),  # dataset path\n",
    "        \"scene\": \"NONE\",  # Scene path\n",
    "        \"default_agent\": 0,\n",
    "        \"sensor_height\": 1.5,  # Height of sensors in meters\n",
    "        \"sensor_pitch\": 0.0,  # sensor pitch (x rotation in rads)\n",
    "        \"color_sensor\": rgb_sensor,  # RGB sensor\n",
    "        \"depth_sensor\": depth_sensor,  # Depth sensor\n",
    "        \"semantic_sensor\": semantic_sensor,  # Semantic sensor\n",
    "        \"seed\": 1,\n",
    "        \"enable_physics\": True,  # enable dynamics simulation\n",
    "    }\n",
    "    return settings\n",
    "\n",
    "\n",
    "def make_simulator_from_settings(sim_settings):\n",
    "    cfg = make_cfg(sim_settings)\n",
    "    # clean-up the current simulator instance if it exists\n",
    "    global sim\n",
    "    global obj_attr_mgr\n",
    "    global prim_attr_mgr\n",
    "    global stage_attr_mgr\n",
    "    global rigid_obj_mgr\n",
    "    global metadata_mediator\n",
    "\n",
    "    if sim != None:\n",
    "        sim.close()\n",
    "    # initialize the simulator\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "    # Managers of various Attributes templates\n",
    "    obj_attr_mgr = sim.get_object_template_manager()\n",
    "    obj_attr_mgr.load_configs(str(os.path.join(data_path, \"objects/example_objects\")))\n",
    "    prim_attr_mgr = sim.get_asset_template_manager()\n",
    "    stage_attr_mgr = sim.get_stage_template_manager()\n",
    "    # Manager providing access to rigid objects\n",
    "    rigid_obj_mgr = sim.get_rigid_object_manager()\n",
    "    # get metadata_mediator\n",
    "    metadata_mediator = sim.metadata_mediator\n",
    "\n",
    "    # UI-populated handles used in various cells.  Need to initialize to valid\n",
    "    # value in case IPyWidgets are not available.\n",
    "    # Holds the user's desired scene handle\n",
    "    global selected_scene\n",
    "    selected_scene = \"NONE\"\n",
    "\n",
    "\n",
    "# [/setup]\n",
    "\n",
    "\n",
    "# %%\n",
    "# @title Define Simulation Utility Function { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "def simulate(sim, dt=1.0, get_frames=True):\n",
    "    # simulate dt seconds at 60Hz to the nearest fixed timestep\n",
    "    print(\"Simulating {:.3f} world seconds.\".format(dt))\n",
    "    observations = []\n",
    "    start_time = sim.get_world_time()\n",
    "    while sim.get_world_time() < start_time + dt:\n",
    "        sim.step_physics(1.0 / 60.0)\n",
    "        if get_frames:\n",
    "            observations.append(sim.get_sensor_observations())\n",
    "    return observations\n",
    "\n",
    "\n",
    "# %%\n",
    "# @title Define GUI Utility Functions { display-mode: \"form\" }\n",
    "# @markdown (double click to show code)\n",
    "\n",
    "# @markdown This cell provides utility functions to build and manage IPyWidget interactive components.\n",
    "\n",
    "\n",
    "# Event handler for dropdowns displaying file-based object handles\n",
    "def on_scene_ddl_change(ddl_values):\n",
    "    global selected_scene\n",
    "    selected_scene = ddl_values[\"new\"]\n",
    "    return selected_scene\n",
    "\n",
    "\n",
    "# Build a dropdown list holding obj_handles and set its event handler\n",
    "def set_handle_ddl_widget(scene_handles, sel_handle, on_change):\n",
    "    descStr = \"Available Scenes:\"\n",
    "    style = {\"description_width\": \"300px\"}\n",
    "    obj_ddl = widgets.Dropdown(\n",
    "        options=scene_handles,\n",
    "        value=sel_handle,\n",
    "        description=descStr,\n",
    "        style=style,\n",
    "        disabled=False,\n",
    "        layout={\"width\": \"max-content\"},\n",
    "    )\n",
    "\n",
    "    obj_ddl.observe(on_change, names=\"value\")\n",
    "    return obj_ddl, sel_handle\n",
    "\n",
    "\n",
    "def set_button_launcher(desc):\n",
    "    button = widgets.Button(\n",
    "        description=desc,\n",
    "        layout={\"width\": \"max-content\"},\n",
    "    )\n",
    "    return button\n",
    "\n",
    "\n",
    "# Builds widget-based UI components\n",
    "def build_widget_ui(metadata_mediator):\n",
    "    # Holds the user's desired scene\n",
    "    global selected_scene\n",
    "\n",
    "    # All file-based object template handles\n",
    "    scene_handles = metadata_mediator.get_scene_handles()\n",
    "    # Set default as first available valid handle, or NONE scene if none are available\n",
    "    if len(scene_handles) == 0:\n",
    "        selected_scene = \"NONE\"\n",
    "    else:\n",
    "        # Set default selection to be first valid non-NONE scene (for python consumers)\n",
    "        for scene_handle in scene_handles:\n",
    "            if \"NONE\" not in scene_handle:\n",
    "                selected_scene = scene_handle\n",
    "                break\n",
    "\n",
    "    if not HAS_WIDGETS:\n",
    "        # If no widgets present, return, using default\n",
    "        return\n",
    "\n",
    "    # Construct DDLs and assign event handlers\n",
    "    # Build widgets\n",
    "    scene_obj_ddl, selected_scene = set_handle_ddl_widget(\n",
    "        scene_handles,\n",
    "        selected_scene,\n",
    "        on_scene_ddl_change,\n",
    "    )\n",
    "\n",
    "    # Display DDLs\n",
    "    ipydisplay(scene_obj_ddl)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--no-display\", dest=\"display\", action=\"store_false\")\n",
    "    parser.add_argument(\"--no-make-video\", dest=\"make_video\", action=\"store_false\")\n",
    "    parser.set_defaults(show_video=True, make_video=True)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    show_video = args.display\n",
    "    display = args.display\n",
    "    make_video = args.make_video\n",
    "else:\n",
    "    show_video = False\n",
    "    make_video = False\n",
    "    display = False\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# Change to do something like this maybe: https://stackoverflow.com/a/41432704\n",
    "def save_sample(rgb_obs, semantic_obs, depth_obs, idx, save_path=dir_path + '/output'):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    # Create and save the RGB image\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "    rgb_img.save(f\"{save_path}/rgb_image_{idx}.png\")\n",
    "\n",
    "    # Create and save the semantic image\n",
    "    semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "    semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "    semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "    semantic_img = semantic_img.convert(\"RGBA\")\n",
    "    semantic_img.save(f\"{save_path}/semantic_image_{idx}.png\")\n",
    "\n",
    "    # Create and save the depth image\n",
    "    depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "    depth_img.save(f\"{save_path}/depth_image_{idx}.png\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # View ReplicaCAD in Habitat-sim\n",
    "# Use the code in this section to view assets in the Habitat-sim engine.\n",
    "\n",
    "# %%\n",
    "# [initialize]\n",
    "# @title Initialize Simulator{ display-mode: \"form\" }\n",
    "\n",
    "sim_settings = make_default_settings()\n",
    "make_simulator_from_settings(sim_settings)\n",
    "# [/initialize]\n",
    "\n",
    "# %%\n",
    "# @title Select a SceneInstance: { display-mode: \"form\" }\n",
    "# @markdown Select a scene from the dropdown and then run the next cell to load and simulate that scene and produce a visualization of the result.\n",
    "\n",
    "build_widget_ui(sim.metadata_mediator)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Load the Select Scene and Simulate!\n",
    "# This cell will load the scene selected above, simulate, and produce a visualization.\n",
    "\n",
    "# %%\n",
    "global selected_scene\n",
    "if sim_settings[\"scene\"] != selected_scene:\n",
    "    sim_settings[\"scene\"] = selected_scene\n",
    "    make_simulator_from_settings(sim_settings)\n",
    "\n",
    "observations = []\n",
    "translations = []\n",
    "rotations = []\n",
    "start_time = sim.get_world_time()\n",
    "count = 1\n",
    "sim.agents[0].scene_node.translation = mn.Vector3([-2, 0, 0])\n",
    "while sim.get_world_time() < start_time + 4.0:\n",
    "    if count < 40:\n",
    "        sim.agents[0].scene_node.rotate(mn.Rad(- mn.math.pi_half / 20.0), mn.Vector3(0, 1, 0))\n",
    "    elif count < 60:\n",
    "        sim.agents[0].scene_node.translation += np.array([0.3, 0, 0])\n",
    "        sim.agents[0].scene_node.rotate(mn.Rad(- mn.math.pi_half / 20.0), mn.Vector3(0, 1, 0))\n",
    "    elif count < 80:\n",
    "        sim.agents[0].scene_node.translation += np.array([0.0, 0, 0.3])\n",
    "        sim.agents[0].scene_node.rotate(mn.Rad(- mn.math.pi_half / 20.0), mn.Vector3(0, 1, 0))\n",
    "    else:\n",
    "        sim.agents[0].scene_node.translation += np.array([-0.1, 0, -0.1])\n",
    "    sim.step_physics(1.0 / 30.0)\n",
    "    if make_video:\n",
    "        observation = sim.get_sensor_observations()\n",
    "        observations.append(observation)\n",
    "        rgb = observation[\"color_sensor_1st_person\"]\n",
    "        semantic = observation[\"semantic_sensor\"]\n",
    "        depth = observation[\"depth_sensor\"]\n",
    "        translation = sim.agents[0].scene_node.translation\n",
    "        rotation = sim.agents[0].scene_node.rotation\n",
    "        translations.append(translation)\n",
    "        rotations.append(rotation)\n",
    "        if display:\n",
    "            save_sample(rgb, semantic, depth, count)\n",
    "            count +=1 \n",
    "\n",
    "# video rendering of carousel view\n",
    "# video_prefix = \"ReplicaCAD_scene_view\"\n",
    "# if make_video:\n",
    "#     vut.make_video(\n",
    "#         observations,\n",
    "#         \"color_sensor_1st_person\",\n",
    "#         \"color\",\n",
    "#         output_path + video_prefix,\n",
    "#         open_vid=show_video,\n",
    "#         video_dims=[1280, 720],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e005f17-2fc6-4e2a-acc0-44953626439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inputs (replace with your data)\n",
    "width, height = 1280, 720\n",
    "fov = 90\n",
    "fx = fy = 0.5 * width / np.tan(np.radians(fov) / 2)\n",
    "cx, cy = width / 2, height / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69198e-299a-4c4d-973d-42a0aa4e058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Camera intrinsics and depth-to-3D conversion function\n",
    "def depth_to_point_cloud(depth, color, fx, fy, cx, cy):\n",
    "    \"\"\"\n",
    "    Convert depth image to point cloud.\n",
    "\n",
    "    Parameters:\n",
    "        depth (ndarray): Depth image (H, W) in meters.\n",
    "        color (ndarray): Color image (H, W, 3).\n",
    "        fx, fy, cx, cy: Intrinsic parameters.\n",
    "        \n",
    "    Returns:\n",
    "        points (ndarray): Point cloud positions (N, 3).\n",
    "        colors (ndarray): Point cloud colors (N, 3).\n",
    "    \"\"\"\n",
    "    height, width = depth.shape\n",
    "    # Create a grid of coordinates corresponding to each pixel\n",
    "    i, j = np.meshgrid(np.arange(width), np.arange(height), indexing='xy')\n",
    "    \n",
    "    # Calculate 3D coordinates in camera space\n",
    "    x = (i - cx) * depth / fx\n",
    "    y = (j - cy) * depth / fy\n",
    "    z = depth\n",
    "\n",
    "    # Mask valid depth values (> 0)\n",
    "    valid = depth > 0\n",
    "    points = np.stack((x[valid], y[valid], z[valid]), axis=-1)\n",
    "    colors = color[valid]\n",
    "    return points, colors\n",
    "\n",
    "# Example inputs (replace with your data)\n",
    "width, height = sim_settings['width'], sim_settings['height']\n",
    "fov = float(sim.agents[0]._sensors['color_sensor_1st_person'].hfov)  # degrees\n",
    "fx = fy = 0.5 * width / np.tan(np.radians(fov) / 2)\n",
    "cx, cy = width / 2, height / 2\n",
    "\n",
    "# Convert to point cloud\n",
    "points, colors = depth_to_point_cloud(depth, rgb[:,:,:3], fx, fy, cx, cy)\n",
    "\n",
    "# Plot point cloud\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=colors / 255.0, s=0.5)\n",
    "ax.set_xlabel(\"X (m)\")\n",
    "ax.set_ylabel(\"Y (m)\")\n",
    "ax.set_zlabel(\"Z (m)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8c22e-e9d5-4ccf-9ffc-530f4bda80e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88eb2d5-216c-4fcb-85b8-cc12e367428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import cv2\n",
    "\n",
    "# Camera intrinsics and optimized depth-to-3D conversion\n",
    "def depth_to_point_cloud_fast(depth, color, fx, fy, cx, cy):\n",
    "    height, width = depth.shape\n",
    "    i, j = np.meshgrid(np.arange(width), np.arange(height), indexing='xy')\n",
    "    x = (i - cx) * depth / fx\n",
    "    y = (j - cy) * depth / fy\n",
    "    z = depth\n",
    "    valid = depth > 0\n",
    "    points = np.stack((x[valid], y[valid], z[valid]), axis=-1)\n",
    "    colors = color[valid]\n",
    "    return points, colors\n",
    "\n",
    "# Example inputs (replace with your data)\n",
    "width, height = 1280, 720\n",
    "fov = 90\n",
    "fx = fy = 0.5 * width / np.tan(np.radians(fov) / 2)\n",
    "cx, cy = width / 2, height / 2\n",
    "\n",
    "\n",
    "# Downsample the depth and color images for performance\n",
    "depth_resized = cv2.resize(depth, (depth.shape[1] // 2, depth.shape[0] // 2))\n",
    "rgb_resized = cv2.resize(rgb, (rgb.shape[1] // 2, rgb.shape[0] // 2))\n",
    "\n",
    "# Convert to point cloud\n",
    "points, colors = depth_to_point_cloud_with_pose(depth, rgb[:,:,:3], fx, fy, cx, cy, translation, rotation)  # depth_to_point_cloud_fast(depth_resized, rgb_resized, fx, fy, cx, cy)\n",
    "\n",
    "\n",
    "# Downsample the point cloud\n",
    "sample_size = int(len(points) * 0.1)  # Keep 10% of the points\n",
    "indices = np.random.choice(len(points), sample_size, replace=False)\n",
    "points_sampled = points[indices]\n",
    "colors_sampled = colors[indices]\n",
    "\n",
    "# Update layout for better appearance and interaction\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"X (m)\",\n",
    "        yaxis_title=\"Y (m)\",\n",
    "        zaxis_title=\"Z (m)\",\n",
    "    ),\n",
    "    title=\"Interactive 3D Point Cloud\",\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points_sampled[:, 0],\n",
    "    y=points_sampled[:, 1],\n",
    "    z=points_sampled[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=1,\n",
    "        color=colors_sampled,  # Use the sampled colors directly from the color image\n",
    "        opacity=0.6\n",
    "    )\n",
    ")])\n",
    "\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e15a0b-b06c-4513-9a91-55f4f9d7a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"X (m)\",\n",
    "        yaxis_title=\"Y (m)\",\n",
    "        zaxis_title=\"Z (m)\",\n",
    "    ),\n",
    "    title=\"Interactive 3D Point Cloud\",\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points_sampled[:, 0],\n",
    "    y=points_sampled[:, 2],\n",
    "    z=- points_sampled[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=1,\n",
    "        color=colors_sampled,  # Use the sampled colors directly from the color image\n",
    "        opacity=0.6\n",
    "    )\n",
    ")])\n",
    "\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1793652-3c23-4694-bb73-d4e1c6a6f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop, merged_points and merged_colors hold the combined point cloud\n",
    "if merged_points:  # Only attempt to merge if there are points\n",
    "    merged_points = np.vstack(merged_points)\n",
    "    merged_colors = np.vstack(merged_colors)\n",
    "    \n",
    "    # Check if merged_points and merged_colors have the correct shapes\n",
    "    if merged_points.shape[0] > 0 and merged_colors.shape[0] > 0:\n",
    "        # Create 3D scatter plot using Plotly's Scatter3d (correct for 3D data)\n",
    "        fig.update_layout(\n",
    "            scene=dict(\n",
    "                xaxis_title=\"X (m)\",\n",
    "                yaxis_title=\"Y (m)\",\n",
    "                zaxis_title=\"Z (m)\",\n",
    "            ),\n",
    "            title=\"Interactive 3D Point Cloud\",\n",
    "            margin=dict(l=0, r=0, b=0, t=40)\n",
    "        )\n",
    "        fig = go.Figure(data=[go.Scatter3d(\n",
    "            x=merged_points[:, 0],\n",
    "            y=merged_points[:, 2],\n",
    "            z=- merged_points[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=1,\n",
    "                color=merged_colors,  # Use the sampled colors directly from the color image\n",
    "                opacity=0.6\n",
    "            )\n",
    "        )])\n",
    "\n",
    "        # Show the interactive plot\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"Merged point cloud data is empty.\")\n",
    "else:\n",
    "    print(\"No points were collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3edbe779-710c-493b-bddb-3e5e86e9534f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7001/static/\n"
     ]
    }
   ],
   "source": [
    "vis = meshcat.Visualizer().open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5ed96844-eead-439a-85a4-25250849a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(-2, 0, 0) Quaternion({0, -0.382683, 0}, 0.923879)\n",
      "Frame 10: 873444 points, 873444 colors\n",
      "Vector(-2, 0, 0) Quaternion({0, -0.707106, 0}, 0.707106)\n",
      "Frame 20: 763843 points, 763843 colors\n",
      "Vector(-2, 0, 0) Quaternion({0, -0.923879, 0}, 0.382683)\n",
      "Frame 30: 712780 points, 712780 colors\n",
      "Vector(-1.7, 0, 0) Quaternion({0, -0.999999, 0}, -1.45286e-07)\n",
      "Frame 40: 796856 points, 796856 colors\n",
      "Vector(1.3, 0, 0) Quaternion({0, -0.923879, 0}, -0.382683)\n",
      "Frame 50: 770338 points, 770338 colors\n",
      "Vector(4, 0, 0.3) Quaternion({0, -0.707106, 0}, -0.707106)\n",
      "Frame 60: 768086 points, 768086 colors\n",
      "Vector(4, 0, 3.3) Quaternion({0, -0.382683, 0}, -0.923878)\n",
      "Frame 70: 681814 points, 681814 colors\n",
      "Vector(3.9, 0, 5.9) Quaternion({0, -0.0392595, 0}, -0.999227)\n",
      "Frame 80: 744792 points, 744792 colors\n",
      "Vector(2.9, 0, 4.9) Quaternion({0, -0.0392595, 0}, -0.999227)\n",
      "Frame 90: 722980 points, 722980 colors\n",
      "Vector(1.9, 0, 3.9) Quaternion({0, -0.0392595, 0}, -0.999227)\n",
      "Frame 100: 715223 points, 715223 colors\n",
      "Vector(0.900001, 0, 2.9) Quaternion({0, -0.0392595, 0}, -0.999227)\n",
      "Frame 110: 743528 points, 743528 colors\n",
      "Vector(-0.0999989, 0, 1.9) Quaternion({0, -0.0392595, 0}, -0.999227)\n",
      "Frame 120: 827190 points, 827190 colors\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import magnum as mn\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Function to convert depth and color to point cloud (with transformation)\n",
    "def depth_to_point_cloud_with_pose(depth, color, fx, fy, cx, cy, translation, rotation):\n",
    "    height, width = depth.shape\n",
    "    i, j = np.meshgrid(np.arange(width), np.arange(height), indexing='xy')\n",
    "    x = (i - cx) * depth / fx\n",
    "    y = (j - cy) * depth / fy\n",
    "    z = depth\n",
    "    valid = depth > 0\n",
    "    points = np.stack((x[valid], y[valid], z[valid]), axis=-1)\n",
    "    colors = color[valid] / 255.0  # Normalize color for Plotly\n",
    "\n",
    "    # Transform points to world coordinates using the agent's pose (translation and rotation)\n",
    "    transformed_points = []\n",
    "    R_image2camera = mn.Matrix3x3(np.array([[1, 0., 0], [0, -1, 0], [0, 0, -1]]))\n",
    "    for point in points:\n",
    "        point_vec = mn.Vector3(point[0], point[1], point[2])\n",
    "        # Apply rotation and then translation\n",
    "        rotated_point = (rotation.to_matrix() @ R_image2camera) * point_vec\n",
    "        world_point = rotated_point + translation  # Apply translation\n",
    "        transformed_points.append(world_point)\n",
    "    \n",
    "    return np.array(transformed_points), colors\n",
    "\n",
    "\n",
    "# Initialize variables for point cloud merging\n",
    "merged_points = []\n",
    "merged_colors = []\n",
    "merged_rotation = []\n",
    "merged_translation = []\n",
    "count = 0\n",
    "for observation, translation, rotation in zip(observations, translations, rotations):\n",
    "    count += 1\n",
    "    if count % 10 != 0:\n",
    "        continue\n",
    "    print(translation, rotation)\n",
    "    # Convert current depth and color image to point cloud\n",
    "    points, colors = depth_to_point_cloud_with_pose(\n",
    "        observation['depth_sensor'], observation['color_sensor_1st_person'][:,:,:3], fx, fy, cx, cy, translation, rotation)\n",
    "    # Downsample the point cloud\n",
    "    sample_size = int(len(points) * 0.005)  # Keep 3% of the points\n",
    "    indices = np.random.choice(len(points), sample_size, replace=False)\n",
    "    points_sampled = points[indices]\n",
    "    colors_sampled = colors[indices]\n",
    "    # Debug: Check if points and colors are generated\n",
    "    print(f\"Frame {count}: {len(points)} points, {len(colors)} colors\")\n",
    "\n",
    "    # Merge point clouds (concatenate the points and colors)\n",
    "    if len(points) > 0:  # Only add if points are generated\n",
    "        merged_points.append(points_sampled)\n",
    "        merged_colors.append(colors_sampled)\n",
    "        merged_rotation.append(rotation)\n",
    "        merged_translation.append(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3db6f418-a64f-4005-98d0-46b5b0e5ada1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector(-2.38895, -2, -2.07193)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_image2camera = mn.Matrix3x3(np.array([[1, 0., 0], [0, -1, 0], [0, 0, -1]]))\n",
    "\n",
    "point_vec = mn.Vector3(1, 2, 3)\n",
    "# Apply rotation and then translation\n",
    "(R_image2camera @ rotation.to_matrix()) * point_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d43c4c35-7340-47ba-9d71-e4670e130987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix(0.92388, 0, -0.382683,\n",
       "       0, -1, 0,\n",
       "       -0.382683, 0, -0.92388)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_image2camera @ rotation.to_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbfe27f1-15a5-467a-9433-1f8f72c5a0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix(0.92388, -0, -0.382683,\n",
       "       0, 1, -0,\n",
       "       0.382683, 0, 0.92388)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation.to_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c7074fb-fc38-41fa-ab23-edf30a72607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import meshcat.geometry as g\n",
    "import meshcat.transformations as tf\n",
    "\n",
    "\n",
    "def draw_meshcat_frame(path: str, rotation, translation):\n",
    "    scale = 0.5\n",
    "    \n",
    "    # Create the X-axis (red)\n",
    "    x_axis = np.array([\n",
    "        [0, scale],  # X-coordinates\n",
    "        [0, 0],      # Y-coordinates\n",
    "        [0, 0]       # Z-coordinates\n",
    "    ])\n",
    "    vis[f\"{path}/x_axis\"].set_object(g.LineSegments(g.PointsGeometry(x_axis), g.MeshBasicMaterial(color=0xff0000)))\n",
    "    \n",
    "    # Create the Y-axis (green)\n",
    "    y_axis = np.array([\n",
    "        [0, 0],      # X-coordinates\n",
    "        [0, scale],  # Y-coordinates\n",
    "        [0, 0]       # Z-coordinates\n",
    "    ])\n",
    "    vis[f\"{path}/y_axis\"].set_object(g.LineSegments(g.PointsGeometry(y_axis), g.MeshBasicMaterial(color=0x00ff00)))\n",
    "    \n",
    "    # Create the Z-axis (blue)\n",
    "    z_axis = np.array([\n",
    "        [0, 0],      # X-coordinates\n",
    "        [0, 0],      # Y-coordinates\n",
    "        [0, scale]   # Z-coordinates\n",
    "    ])\n",
    "    vis[f\"{path}/z_axis\"].set_object(g.LineSegments(g.PointsGeometry(z_axis), g.MeshBasicMaterial(color=0x0000ff)))\n",
    "    \n",
    "    # Optional: Apply a transformation to the coordinate frame\n",
    "    rotation_matrix = np.eye(4)\n",
    "    rotation_matrix[:3, :3] = rotation.to_matrix()\n",
    "    transform = tf.concatenate_matrices(\n",
    "        tf.translation_matrix(translation),\n",
    "        rotation_matrix,\n",
    "    )\n",
    "    vis[path].set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61b69ef3-2cfc-459c-9cef-bc9a3839b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (points, colors, rotation ,translation) in enumerate(zip(merged_points, merged_colors, merged_rotation, merged_translation)):\n",
    "    vis[f\"view{i}/pointcloud\"].set_object(g.PointCloud(points.T, colors.T, size=0.03))\n",
    "    draw_meshcat_frame(f\"view{i}/frame\", rotation, np.array([translation.x, translation.y, translation.z]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29e800be-4376-4c26-a266-d3e442f17641",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (rotation ,translation) in enumerate(zip(rotations, translations)):\n",
    "    draw_meshcat_frame(f\"path/view{i}/frame\", rotation, np.array([translation.x, translation.y, translation.z]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "habitat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
